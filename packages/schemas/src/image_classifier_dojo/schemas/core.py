from typing import List, Optional, Union, Literal, Annotated

from pydantic import BaseModel, Field


# Custom Hydra Yaml #

class HydraYamlConfig(BaseModel):
    content: str = Field(..., description="Raw YAML content as a string")
    filename: Optional[str] = Field('config.yaml', description="Optional filename for the saved YAML content")
    save_path: Optional[str] = Field('/tmp', description="Optional path to save the YAML content")

# =========================
# DATASET
# =========================
class ListfileDatasetConfig(BaseModel):
    classlist: str = Field(...,
        description="A text file, each line is a class label (the label order is significant)")
    trainlist: str = Field(...,
        description="A text file, one sample per line, each sample has a class-index and image path")
    vallist: str = Field(...,
        description="Like trainlist, but for validation metrics and early-stopping/overfit-prevention")
    testlist: Optional[str] = Field(None,
        description="Like trainlist, but for final test metrics. Optional")
    # sampler: Optional[str] = None  # e.g., "module.Class"
    # is_batch_sampler: bool = False
    # feature_store: Optional[str] = None
    # dataset: Optional[str] = None


# =========================
# TRAINING AUGMENTATION
# =========================

class NormalizeConfig(BaseModel):
    mean: Union[float,List[float]] = Field([0.485, 0.456, 0.406],
        description="Mean values for each channel used in normalization. Defaults to ImageNet standards")
    std: Union[float,List[float]] = Field([0.229, 0.224, 0.225],
        description="Standard deviation values for each channel used in normalization. Defaults to ImageNet standards")

class BaseAugmentationConfig(BaseModel):
    resize: Optional[int] = Field(None,
        description="Resize images to (size,size). If left blank, gets auto-calculated.")
    normalize: Optional[NormalizeConfig] = Field(None,
        description="If True, normalizes images to ImageNet standards")

class TrainingAugmentationConfig(BaseModel):
    # Accepts 'x', 'y', 'xy'
    flip: Optional[Literal["x", "y", "xy"]] = Field(None,
        description=(
            "Training images have 50% chance of being flipped along the designated axis: "
            "(x) vertically, (y) horizontally, (xy) either/both."
        ))



# =========================
# TRACKING (AimLogger)
# =========================

class S3UriConfig(BaseModel):
    uri: str = Field(..., description='Full S3 URI', example='s3://MyBucket/prefix')
    endpoint: str = Field(..., description='S3 Endpoint for ')
    accesskey: str = Field(..., description='Access Key')
    secretkey: str = Field(..., description='Secret Key')

class S3Config(BaseModel):
    bucket: str = Field(..., description='S3 Bucket Name')
    prefix: Optional[str] = Field('', description='Optional Prefix/Folder in the Bucket')
    endpoint: str = Field(..., description='S3 Endpoint for ')
    accesskey: str = Field(..., description='Access Key')
    secretkey: str = Field(..., description='Secret Key')

class AimLoggerConfig(BaseModel):
    repo: str = Field(...,
        description="Aim repo path. Also see: Aim environment variables.")
    run: Optional[str] = Field(None,
        description="The name of this run. A run name is automatically generated by default")
    experiment: Optional[str] = Field(None,
        description="The broader category/grouping this RUN belongs to")
    note: Optional[str] = Field(None,
        description='Add any kind of note or description to the trained model.')
    artifacts_location: Optional[Union[str,S3UriConfig,S3Config]] = Field(None,
        description="Aim Artifacts location. Either a local path or an S3 location")
    # todo plot / callback / metric


# =========================
# MODEL CONFIG
# =========================
class ModelBackboneConfig(BaseModel):
    # model is required
    model_name: str = Field(..., description="Model Class/Module Name")

    pretrained_weights: Optional[str,None] = Field("DEFAULT",
        description='''Specify a model's downloadable pretrained weights. 
Either "DEFAULT", some specific identifier, or "None" for no-pretrained-weights''')

class ModelHeadConfig(BaseModel):
    head_type: str = Field(..., description='The type of model output. May be "multiclass", "regression", "embedding"')
    name: Optional[str] = Field('head', description='An optional name for this head, if multiple heads are used')

class MulticlassHeadConfig(ModelHeadConfig):
    head_type: Literal['multiclass'] = 'multiclass'
    num_classes: int = Field(..., description='The number of classes to predict')

class EmbeddingsHeadConfig(ModelHeadConfig):
    head_type: Literal['embeddings'] = 'embeddings'
    embedding_size: list[int] = Field(..., description='The size of the output embedding vector')

class RegressionHeadConfig(ModelHeadConfig):
    head_type: Literal['regression'] = 'regression'
    range_min: Optional[float] = Field(None, description='The minimum possible value of the output')
    range_max: Optional[float] = Field(None, description='The maximum possible value of the output')

ModelHeadConfigType = Annotated[Union[MulticlassHeadConfig, EmbeddingsHeadConfig, RegressionHeadConfig],
                                Field(discriminator='head_type')]

class ModelConfig(BaseModel):
    backbone: ModelBackboneConfig = Field(..., description='The model backbone configuration')
    head: ModelHeadConfigType = Field(..., description='The model head (output) configuration')
    weights: Optional[str] = Field(None, description='Path to model weights/checkpoint to load')

class MultiheadModelConfig(BaseModel):
    backbone: ModelBackboneConfig = Field(..., description='The model backbone configuration')
    heads: List[ModelHeadConfigType] = Field(..., description='A list of model heads (outputs). At least one head is required.')

# =========================
# TRAINING CONFIG
# =========================

## Loss Functions ##

class LossFunctionConfig(BaseModel):
    loss_function: str = Field(..., description='The Loss Function')

class MulticlassLossFunctionConfig(LossFunctionConfig):
    loss_function: Literal['CrossEntropyLoss', 'FocalLoss']
    ignore_index: Optional[int] = Field(None,
        description='Specifies a target value that is ignored and does not contribute to the loss / input gradient')
    reduction: Literal['sum','mean','none'] = Field('sum',
        description='How loss for each input gets aggregated.')

class CrossEntropyLossConfig(MulticlassLossFunctionConfig):
    loss_function: Literal['CrossEntropyLoss'] = 'CrossEntropyLoss'
    smoothing: float = Field(0.0, examples=[0.1, 0.2],
        description='Label Smoothing Regularization arg. Range is 0-1. Default is 0')
    weights: Optional[Union[Literal['normalize'], list[float]]] = Field(None,
        description='If "normalize", rare class instances will be boosted. '
                    'Otherwise accepts a list of per-class weights. Length must match number of classes')

class FocalLossConfig(MulticlassLossFunctionConfig):
    loss_function: Literal['FocalLoss'] = 'FocalLoss'
    gamma: float = Field(1.0, examples=[1.0,2.0,5.0],
        description='Rate at which easy examples are down-weighted')
    alpha: Optional[list[float]] = Field(None,
        description='A list of per-class weights. Length must match number of classes')

LossConfig = Annotated[Union[CrossEntropyLossConfig, FocalLossConfig], Field(discriminator='loss_function')]

## Optimizers ##

class OptimizerConfig(BaseModel):
    # Discriminator field used to pick the concrete config
    optimizer: str = Field(..., description='Optimizer. Eg: "AdamW". Default is "Adam"')
    lr: float = Field(0.001, description="Initial Learning Rate. Default is 0.001")


class AdamConfig(OptimizerConfig):
    optimizer: Literal["Adam"] = "Adam"
    amsgrad: bool = Field(False, description="Use the AMSGrad variant of Adam")
    #betas: tuple[float, float] = Field((0.9, 0.999), description="Adam beta coefficients (beta1, beta2)")
    #eps: float = Field(1e-8, description="Term added to the denominator for numerical stability")
    #weight_decay: float = Field(0.0, description="L2 penalty (weight decay)")
    # foreach: Optional[bool] = None,
    # maximize: bool = False,
    # capturable: bool = False,
    # differentiable: bool = False,
    # fused: Optional[bool] = None,
    # decoupled_weight_decay: bool = False,

class AdamWConfig(OptimizerConfig):
    optimizer: Literal["AdamW"] = "AdamW"
    amsgrad: bool = Field(False, description="Use the AMSGrad variant of AdamW")

class SGDConfig(OptimizerConfig):
    optimizer: Literal["SGD"] = "SGD"
    momentum: float = Field(0.0, description="SGD momentum factor")
    dampening: float = Field(0.0, description="SGD dampening for momentum")
    weight_decay: float = Field(0.0, description="L2 penalty (weight decay)")
    nesterov: bool = Field(False, description="Enable Nesterov momentum")

OptimConfig = Annotated[
    Union[AdamConfig, AdamWConfig, SGDConfig],
    Field(discriminator="optimizer"),
]

## SWA ##
# import lightning.pytorch.callbacks.stochastic_weight_avg
# https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
# see https://chatgpt.com/c/68f138d0-7fec-8327-8124-f30c9abbc297
#     Option A — Lightning-native or Option B — Post-hoc SWA with PyTorch utilities
class SWACallbackConfig(BaseModel):
    swa_lr: float = Field(0.0003, description="Stochastic Weight Averaging learning rate. Should be 0.1 to 0.3 of the global learning rate (typically 0.001)")
    swa_epoch_start: Union[int, float] = Field(0,
        description = "If provided as int, the procedure will start from the ``swa_epoch_start``-th epoch."
                      "If provided as float between 0 and 1, the procedure will start from ``int(swa_epoch_start * max_epochs)`` epoch")
    annealing_epochs: int = Field(5, description="Reduces the learning rate to swa_lr over these many epochs")
    annealing_strategy: Literal["cos", "linear"] = Field("cos", description="Stochastic Weight Averaging annealing strategy")

class SWAPolishConfig(BaseModel):
    swa_lr: float = Field(0.0003, description="Stochastic Weight Averaging learning rate during polishing. ")
    epochs: int = Field(5, description='Number of epochs to run SWA polishing for at the end of training. 2-5 epochs should be fine', examples=[2,5])
    sgd_momentum: float = Field(0.9, description="SGD momentum factor for SWA polishing")


## Epochs ##

class EpochConfig(BaseModel):
    epoch_max: int = Field(100, description="Maximum number of training epochs. Default is 100")
    epoch_min: Optional[int] = Field(None, description="Minimum number of training epochs. Default is 10")
    epoch_stop: Optional[int] = Field(None,
        description="Early Stopping: Number of epochs following a best-epoch after-which to stop training")


## TRAINER CONFIG ##

class TrainerConfig(BaseModel):

    epochs_config: EpochConfig = Field(default_factory=EpochConfig, description='Epoch configuration')
    batch_size: int = Field(256, description="Number of images per batch. Defaults is 256")
    freeze: Optional[str] = Field(None,
        description=
            "Freezes a models leading feature layers. "
            "Positive int freezes the first N layers/features/blocks. A negative int like '-1' freezes all but the last N feature/layer/block. "
            "A positive float like '0.8' freezes the leading 80% of features/layers/blocks. fc or final_classifier layers are never frozen."
        ) # todo everything prior-to a named layer

    loss_config: LossConfig = Field(default_factory=CrossEntropyLossConfig,
        description="Loss Function.")

    optimizer_config: Union[OptimConfig, HydraYamlConfig] = Field(default_factory=AdamConfig,
                                                                  description="Optimizer configuration. Defaults to Adam with lr=0.001", )

    # todo learning rate scheduler config

    # Keep as free string to avoid coupling to a specific PL enum; default matches your argparse
    precision: str = Field("16-mixed",
        description='Precision. Default is "16-mixed" for mixed precision')

    swa_config: Optional[SWACallbackConfig,SWAPolishConfig] = Field(None,
        description="Stochastic Weight Averaging (SWA) configuration. If provided, enables SWA training or SWA best-epoch polishing.")

    # ensemble: Optional[str] = Field(None, description="Model Ensembling mode")


# =========================
# UTILITIES / MISC
# =========================

class AutoBatchConfig(BaseModel):
    mode: Literal["power", "binsearch"] = Field('power', description="Power-of-2 scaling or binary search")
    max_size: int = Field(1024, description="batch size limit")
    min_size: int = Field(32, description="batch size minimum. Error is raised if autobatch tries to go lower.")


class ONNXConfig(BaseModel):
    opset: int = Field(11, description="ONNX opset version to export the model with.")
    half: bool = Field(False, description="Export the model with half-precision (float16)")
    device: Literal["cpu", "cuda"] = Field("cpu", description="Device to use for the export")
    batch_size: Optional[int] = Field(None, description="Batch size to use for the export. If not set, 'dynamic' batch sizing is used")
    input_names: List[str] = Field(["input"], description="Names to assign to the input nodes of the graph.")
    output_names: List[str] = Field(["output"], description="Names to assign to the output nodes of the graph.")
    # todo filename/outdir/filepattern/artifact

class TrainingParams(BaseModel):
    checkpoints_path: str = "./experiments"
    seed: Optional[int] = Field(None, description="Set a specific seed for deterministic output")
    num_workers: int = Field(4, description="Number of data-loading threads. 4 per GPU is typical")
    autobatch: Optional[AutoBatchConfig] = Field(None, description="Auto-Tunes batch_size prior to training/inference.")
    fast_dev_run: Optional[bool] = Field(False, description="Runs a single batch of train, val, and test to find any bugs. Default is False")
    gpus: Union[Literal['cpu','cuda','CUDA_VISIBLE_DEVICES'], List[int]] = Field('cuda',
        description='Which GPU(s) to use, by ID. Default is "cuda" (equivalent to "CUDA_VISIBLE_DEVICES"). Use "cpu" for CPU only.')
    onnx: Optional[List[ONNXConfig]] = Field(None, description="If provided, exports the trained model to ONNX")


# =========================
# COMPOSED TASK CONFIG (handy for end-to-end configs)
# =========================
class TrainingRunConfig(BaseModel):
    aim_config: AimLoggerConfig
    dataset_config: ListfileDatasetConfig
    model_config: ModelConfig
    training_params: TrainingParams
    trainer_config: TrainerConfig
    base_augs: BaseAugmentationConfig
    train_augs: TrainingAugmentationConfig
    # plotting callbacks



